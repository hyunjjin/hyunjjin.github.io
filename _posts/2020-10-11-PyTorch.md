---
title: Python - PyTorch Tensor, Autograd
tags: python ML
---


# 1. PyTorch - Tensor

### < 신경망의 순전파, 역전파 단계 구현 >
- NumPy로는 최근 딥러닝 연산을 가속화시킬 수 없다.
- Tensor는 NumPy와 같이, 연산을 위한 포괄적인 도구다.
- GPU를 활용하여 수치연산 가속화 가능하다.


```python
# https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/pytorch_with_examples.html
    
import torch
```


```python
# 데이터 타입
# https://pytorch.org/docs/stable/tensors.html

dtype = torch.FloatTensor # CPU tensor
dtype
```




    torch.FloatTensor




```python
# N; 배치크기, D_in; 입력차원, H; 은닉계층차원, D_out; 출력차원

N, D_in, H, D_out = 64, 1000, 100, 10
```


```python
# 입출력 데이터

x = torch.randn(N, D_in).type(dtype)
y = torch.randn(N, D_out).type(dtype)
print(x.shape, y.shape)
```

    torch.Size([64, 1000]) torch.Size([64, 10])



```python
# 무작위 가중치 초기화

w1 = torch.randn(D_in, H).type(dtype)
w2 = torch.randn(H, D_out).type(dtype)
print(w1.shape, w2.shape)
```

    torch.Size([1000, 100]) torch.Size([100, 10])



```python
learning_rate = 1e-6
for t in range(500):
    
    # 순전파 단계; predict y
    h=x.mm(w1) # matrix multiplication
    h_relu = h.clamp(min=0) # 0이하인 것은 0으로 대체
    y_pred = h_relu.mm(w2)
    
    # calculate loss
    loss = (y_pred - y).pow(2).sum()
    print(t, loss)
    
    # loss에 따른 w1, w2의 변화도 계산하고 역전파
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = x.t().mm(grad_h)
    
    # gradient descent 이용해 가중치 갱신
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```




----

# 2. PyTorch - Autograd
- 위에서 역전파를 수동으로 구현하였는데, 이를 자동미분을 사용해 연산을 자동화 할 수 있다.


```python
import torch
from torch.autograd import Variable
```


```python
dtype = torch.FloatTensor
N, D_in, H, D_out = 64, 1000, 100, 10
```


```python
# 입출력 데이터

x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False) # requires_grad; 변화도 계산
y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)
print(x.shape, y.shape)


# 무작위 가중치 초기화

w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)
w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)
print(w1.shape, w2.shape)
```

    torch.Size([64, 1000]) torch.Size([64, 10])
    torch.Size([1000, 100]) torch.Size([100, 10])



```python
learning_rate = 1e-6
for t in range(500):
    
    # 순전파 단계; predict y
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    
    # calculate loss
    loss = (y_pred - y).pow(2).sum()
    print(t, loss.data)
    
    # 역전파 단계; autograde
    loss.backward()
    
    # gradient descent 이용해 가중치 갱신
    w1.data -= learning_rate * w1.grad.data
    w2.data -= learning_rate * w2.grad.data
    
    # 수동으로 변화도=0으로 만들기
    w1.grad.data.zero_()
    w2.grad.data.zero_()
```





```python

```
